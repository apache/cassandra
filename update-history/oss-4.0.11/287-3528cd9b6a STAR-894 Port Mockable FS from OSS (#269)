--- a/src/java/org/apache/cassandra/db/Directories.java
+++ b/src/java/org/apache/cassandra/db/Directories.java
@@ -42,12 +42,8 @@
 import org.apache.cassandra.io.FSNoDiskAvailableForWriteError;
 import org.apache.cassandra.io.FSWriteError;
 import org.apache.cassandra.io.sstable.*;
-<<<<<<<
 import org.apache.cassandra.io.sstable.SnapshotDeletingTask;
-import org.apache.cassandra.io.sstable.format.SSTableReader;
-=======
 import org.apache.cassandra.io.util.File;
->>>>>>>
 import org.apache.cassandra.io.util.FileUtils;
 import org.apache.cassandra.io.util.PathUtils;
 import org.apache.cassandra.schema.SchemaConstants;
@@ -512,7 +508,7 @@
             StringJoiner pathString = new StringJoiner(",", "[", "]");
             for (DataDirectory p: paths)
             {
-                pathString.add(p.location.getAbsolutePath());
+                pathString.add(p.location.absolutePath());
             }
             logger.warn("Insufficient disk space for compaction. Across {} there's only {} available, but {} is needed.",
                         pathString.toString(),
--- a/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
+++ b/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
@@ -27,7 +27,6 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Optional;
 import java.util.TreeMap;
 import java.util.UUID;
 import java.util.function.BiPredicate;
@@ -42,7 +41,6 @@
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.config.ParameterizedClass;
 import org.apache.cassandra.db.Mutation;
-import org.apache.cassandra.db.SystemKeyspace;
 import org.apache.cassandra.exceptions.CDCWriteException;
 import org.apache.cassandra.io.FSWriteError;
 import org.apache.cassandra.io.compress.ICompressor;
@@ -78,7 +76,7 @@
 
     public static final CommitLog instance = CommitLog.construct();
 
-    private static final FilenameFilter unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);
+    private static final BiPredicate<File, String> unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);
 
     final public AbstractCommitLogSegmentManager segmentManager;
 
@@ -172,7 +170,7 @@
 
     private File[] getUnmanagedFiles()
     {
-        File[] files = new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter);
+        File[] files = new File(segmentManager.storageDirectory).tryList(unmanagedFilesFilter);
         if (files == null)
             return new File[0];
         return files;
@@ -186,19 +184,10 @@
      */
     public int recoverSegmentsOnDisk() throws IOException
     {
-<<<<<<<
         // submit all files for this segment manager for archiving prior to recovery - CASSANDRA-6904
         // The files may have already been archived by normal CommitLog operation. This may cause errors in this
         // archiving pass, which we should not treat as serious.
         for (File file : getUnmanagedFiles())
-=======
-        BiPredicate<File, String> unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);
-
-        // submit all files for this segment manager for archiving prior to recovery - CASSANDRA-6904
-        // The files may have already been archived by normal CommitLog operation. This may cause errors in this
-        // archiving pass, which we should not treat as serious.
-        for (File file : new File(segmentManager.storageDirectory).tryList(unmanagedFilesFilter))
->>>>>>>
         {
             archiver.maybeArchive(file.path(), file.name());
             archiver.maybeWaitForArchiving(file.name());
@@ -208,11 +197,7 @@
         archiver.maybeRestoreArchive();
 
         // List the files again as archiver may have added segments.
-<<<<<<<
         File[] files = getUnmanagedFiles();
-=======
-        File[] files = new File(segmentManager.storageDirectory).tryList(unmanagedFilesFilter);
->>>>>>>
         int replayed = 0;
         if (files.length == 0)
         {
--- a/src/java/org/apache/cassandra/io/util/FileUtils.java
+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java
@@ -63,18 +63,11 @@
 import org.apache.cassandra.io.FSErrorHandler;
 import org.apache.cassandra.io.FSWriteError;
 import org.apache.cassandra.io.sstable.CorruptSSTableException;
-<<<<<<<
-=======
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.ByteBufferUtil;
-import org.apache.cassandra.utils.FBUtilities;
->>>>>>>
 import org.apache.cassandra.utils.JVMStabilityInspector;
 import org.apache.cassandra.utils.SyncUtil;
 
 import static com.google.common.base.Throwables.propagate;
 import static org.apache.cassandra.config.CassandraRelevantProperties.JAVA_IO_TMPDIR;
-import static org.apache.cassandra.config.CassandraRelevantProperties.USE_NIX_RECURSIVE_DELETE;
 import static org.apache.cassandra.utils.Throwables.maybeFail;
 
 public final class FileUtils
@@ -471,137 +464,6 @@
         }
     }
 
-<<<<<<<
-=======
-    /**
-     * Deletes all files and subdirectories under "dir".
-     * @param dir Directory to be deleted
-     * @throws FSWriteError if any part of the tree cannot be deleted
-     */
-    public static void deleteRecursiveWithThrottle(File dir, RateLimiter rateLimiter)
-    {
-        if (dir.isDirectory())
-        {
-            String[] children = dir.list();
-            for (String child : children)
-                deleteRecursiveWithThrottle(new File(dir, child), rateLimiter);
-        }
-
-        // The directory is now empty so now it can be smoked
-        deleteWithConfirmWithThrottle(dir, rateLimiter);
-    }
-
-
-    /**
-     * Deletes the specified directory after having deleted its content.
-     *
-     * @param dir Directory to be deleted
-     * @throws FSWriteError if any part of the tree cannot be deleted
-     */
-    public static void deleteRecursive(File dir)
-    {
-        if (USE_NIX_RECURSIVE_DELETE.getBoolean() && dir.toPath().getFileSystem() == FileSystems.getDefault())
-        {
-            deleteRecursiveUsingNixCommand(dir.toPath(), false);
-            return;
-        }
-
-        deleteChildrenRecursive(dir);
-
-        // The directory is now empty, so now it can be smoked
-        deleteWithConfirm(dir);
-    }
-
-    /**
-     * Deletes all files and subdirectories under "dir".
-     *
-     * @param dir Directory to be deleted
-     * @throws FSWriteError if any part of the tree cannot be deleted
-     */
-    public static void deleteChildrenRecursive(File dir)
-    {
-        if (dir.isDirectory())
-        {
-            String[] children = dir.list();
-            if (children.length == 0)
-                return;
-
-            if (USE_NIX_RECURSIVE_DELETE.getBoolean() && dir.toPath().getFileSystem() == FileSystems.getDefault())
-            {
-                for (String child : children)
-                    deleteRecursiveUsingNixCommand(dir.toPath().resolve(child), false);
-            }
-            else
-            {
-                for (String child : children)
-                    deleteRecursive(new File(dir, child));
-            }
-        }
-    }
-
-    /**
-     * Uses unix `rm -r` to delete a directory recursively.
-     * This method can be much faster than deleting files and directories recursively by traversing them with Java.
-     * Though, we use it only for tests because it provides less information about the problem when something goes wrong.
-     *
-     * @param path        path to be deleted
-     * @param quietly     if quietly, additional `-f` flag is added to the `rm` command so that it will not complain in case
-     *                    the provided path is missing
-     */
-    private static void deleteRecursiveUsingNixCommand(Path path, boolean quietly)
-    {
-        String[] cmd = new String[]{ "rm",
-                                     quietly ? "-drf" : "-dr",
-                                     path.toAbsolutePath().toString() };
-
-        try
-        {
-            Process p = Runtime.getRuntime().exec(cmd);
-            int result = p.waitFor();
-
-            String out, err;
-            try (BufferedReader outReader = new BufferedReader(new InputStreamReader(p.getInputStream()));
-                 BufferedReader errReader = new BufferedReader(new InputStreamReader(p.getErrorStream())))
-            {
-                out = outReader.lines().collect(Collectors.joining("\n"));
-                err = errReader.lines().collect(Collectors.joining("\n"));
-            }
-
-            if (result != 0 && Files.exists(path))
-            {
-                logger.error("{} returned:\nstdout:\n{}\n\nstderr:\n{}", Arrays.toString(cmd), out, err);
-                throw new IOException(String.format("%s returned non-zero exit code: %d%nstdout:%n%s%n%nstderr:%n%s", Arrays.toString(cmd), result, out, err));
-            }
-        }
-        catch (IOException e)
-        {
-            throw new FSWriteError(e, path.toString());
-        }
-        catch (InterruptedException e)
-        {
-            Thread.currentThread().interrupt();
-            throw new FSWriteError(e, path.toString());
-        }
-    }
-
-    /**
-     * Schedules deletion of all file and subdirectories under "dir" on JVM shutdown.
-     * @param dir Directory to be deleted
-     */
-    public static void deleteRecursiveOnExit(File dir)
-    {
-        if (dir.isDirectory())
-        {
-            String[] children = dir.list();
-            for (String child : children)
-                deleteRecursiveOnExit(new File(dir, child));
-        }
-
-        logger.trace("Scheduling deferred deletion of file: {}", dir);
-        dir.deleteOnExit();
-    }
-
->>>>>>>
     public static void handleCorruptSSTable(CorruptSSTableException e)
     {
         fsErrorHandler.get().ifPresent(handler -> handler.handleCorruptSSTable(e));
--- a/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
+++ b/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
@@ -18,13 +18,9 @@
 
 package org.apache.cassandra.service;
 
-<<<<<<<
-=======
-import java.io.File;
 import java.util.Set;
 
 import com.google.common.collect.ImmutableSet;
->>>>>>>
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -71,7 +67,6 @@
 
         switch (DatabaseDescriptor.getDiskFailurePolicy())
         {
-            case die:
             case stop_paranoid:
             case stop:
                 // exception not logged here on purpose as it is already logged
@@ -92,13 +87,8 @@
                 }
 
                 // for both read and write errors mark the path as unwritable.
-<<<<<<<
-                DisallowedDirectories.maybeMarkUnwritable(e.path);
-                if (e instanceof FSReadError && shouldMaybeRemoveData(e))
-=======
                 DisallowedDirectories.maybeMarkUnwritable(new File(e.path));
-                if (e instanceof FSReadError)
->>>>>>>
+                if (e instanceof FSReadError && shouldMaybeRemoveData(e))
                 {
                     File directory = DisallowedDirectories.maybeMarkUnreadable(new File(e.path));
                     if (directory != null)
--- a/test/distributed/org/apache/cassandra/distributed/test/ResourceLeakTest.java
+++ b/test/distributed/org/apache/cassandra/distributed/test/ResourceLeakTest.java
@@ -39,12 +39,9 @@
 import org.apache.cassandra.distributed.api.ConsistencyLevel;
 import org.apache.cassandra.distributed.api.Feature;
 import org.apache.cassandra.distributed.api.IInstanceConfig;
-<<<<<<<
 import org.apache.cassandra.distributed.api.IInvokableInstance;
 import org.apache.cassandra.distributed.shared.JMXUtil;
-=======
 import org.apache.cassandra.io.util.File;
->>>>>>>
 import org.apache.cassandra.utils.SigarLibrary;
 
 import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
--- a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
+++ b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
@@ -25,21 +25,17 @@
 import java.util.*;
 
 import com.google.common.collect.Iterators;
-<<<<<<<
-=======
 import com.googlecode.concurrenttrees.common.Iterables;
 import org.apache.cassandra.db.memtable.Memtable;
 import org.json.simple.JSONArray;
 import org.json.simple.JSONObject;
 import org.json.simple.parser.JSONParser;
->>>>>>>
 import org.junit.Assert;
 import org.junit.Assume;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
-import com.googlecode.concurrenttrees.common.Iterables;
 import org.apache.cassandra.SchemaLoader;
 import org.apache.cassandra.UpdateBuilder;
 import org.apache.cassandra.Util;
@@ -62,9 +58,6 @@
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.WrappedRunnable;
-import org.json.simple.JSONArray;
-import org.json.simple.JSONObject;
-import org.json.simple.parser.JSONParser;
 
 import static junit.framework.Assert.assertNotNull;
 import static org.apache.cassandra.db.ColumnFamilyStore.FlushReason.UNIT_TESTS;
--- a/test/unit/org/apache/cassandra/db/DirectoriesTest.java
+++ b/test/unit/org/apache/cassandra/db/DirectoriesTest.java
@@ -20,10 +20,7 @@
 import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
-<<<<<<<
-import java.util.*;
 import java.util.concurrent.atomic.AtomicInteger;
-=======
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
@@ -35,10 +32,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
->>>>>>>
 import java.util.concurrent.Callable;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.function.Supplier;
@@ -47,11 +41,8 @@
 
 import com.google.common.collect.Sets;
 import org.apache.commons.lang3.StringUtils;
-<<<<<<<
-=======
 
 import org.junit.After;
->>>>>>>
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
--- a/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java
+++ b/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java
@@ -45,15 +45,10 @@
 import org.apache.cassandra.cql3.CQLTester;
 import org.apache.cassandra.cql3.QueryOptions;
 import org.apache.cassandra.cql3.statements.BatchStatement;
-<<<<<<<
-=======
 import org.apache.cassandra.exceptions.ConfigurationException;
 import org.apache.cassandra.fql.FullQueryLogger.Query;
 import org.apache.cassandra.fql.FullQueryLogger.Batch;
->>>>>>>
 import org.apache.cassandra.cql3.statements.BatchStatement.Type;
-import org.apache.cassandra.fql.FullQueryLogger.Batch;
-import org.apache.cassandra.fql.FullQueryLogger.Query;
 import org.apache.cassandra.io.util.File;
 import org.apache.cassandra.io.util.FileUtils;
 import org.apache.cassandra.service.ClientState;
@@ -76,13 +71,10 @@
 import static org.apache.cassandra.fql.FullQueryLogger.TYPE;
 import static org.apache.cassandra.fql.FullQueryLogger.VALUES;
 import static org.apache.cassandra.fql.FullQueryLogger.VERSION;
-<<<<<<<
+import static org.junit.Assert.fail;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
-=======
-import static org.junit.Assert.fail;
->>>>>>>
 
 public class FullQueryLoggerTest extends CQLTester
 {
diff --git a/src/java/org/apache/cassandra/db/Directories.java b/src/java/org/apache/cassandra/db/Directories.java
index 0dbf0d7d34..0987b607bb 100644
--- a/src/java/org/apache/cassandra/db/Directories.java
+++ b/src/java/org/apache/cassandra/db/Directories.java
@@ -508,7 +508,7 @@ public class Directories
             StringJoiner pathString = new StringJoiner(",", "[", "]");
             for (DataDirectory p: paths)
             {
-                pathString.add(p.location.getAbsolutePath());
+                pathString.add(p.location.absolutePath());
             }
             logger.warn("Insufficient disk space for compaction. Across {} there's only {} available, but {} is needed.",
                         pathString.toString(),
diff --git a/src/java/org/apache/cassandra/db/commitlog/CommitLog.java b/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
index 9bbb23536d..40f19d0805 100644
--- a/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
+++ b/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
@@ -27,7 +27,6 @@ import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Optional;
 import java.util.TreeMap;
 import java.util.UUID;
 import java.util.function.BiPredicate;
@@ -42,7 +41,6 @@ import org.slf4j.LoggerFactory;
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.config.ParameterizedClass;
 import org.apache.cassandra.db.Mutation;
-import org.apache.cassandra.db.SystemKeyspace;
 import org.apache.cassandra.exceptions.CDCWriteException;
 import org.apache.cassandra.io.FSWriteError;
 import org.apache.cassandra.io.compress.ICompressor;
@@ -78,7 +76,7 @@ public class CommitLog implements CommitLogMBean
 
     public static final CommitLog instance = CommitLog.construct();
 
-    private static final FilenameFilter unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);
+    private static final BiPredicate<File, String> unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);
 
     final public AbstractCommitLogSegmentManager segmentManager;
 
@@ -172,7 +170,7 @@ public class CommitLog implements CommitLogMBean
 
     private File[] getUnmanagedFiles()
     {
-        File[] files = new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter);
+        File[] files = new File(segmentManager.storageDirectory).tryList(unmanagedFilesFilter);
         if (files == null)
             return new File[0];
         return files;
diff --git a/src/java/org/apache/cassandra/io/util/FileUtils.java b/src/java/org/apache/cassandra/io/util/FileUtils.java
index a146333dc5..62456a366b 100644
--- a/src/java/org/apache/cassandra/io/util/FileUtils.java
+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java
@@ -68,7 +68,6 @@ import org.apache.cassandra.utils.SyncUtil;
 
 import static com.google.common.base.Throwables.propagate;
 import static org.apache.cassandra.config.CassandraRelevantProperties.JAVA_IO_TMPDIR;
-import static org.apache.cassandra.config.CassandraRelevantProperties.USE_NIX_RECURSIVE_DELETE;
 import static org.apache.cassandra.utils.Throwables.maybeFail;
 
 public final class FileUtils
diff --git a/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java b/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
index 9ff3a746a6..a911b6f806 100644
--- a/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
+++ b/src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
@@ -67,7 +67,6 @@ public class DefaultFSErrorHandler implements FSErrorHandler
 
         switch (DatabaseDescriptor.getDiskFailurePolicy())
         {
-            case die:
             case stop_paranoid:
             case stop:
                 // exception not logged here on purpose as it is already logged
diff --git a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
index 5f3aa3cb27..f8bf4aa5a4 100644
--- a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
+++ b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
@@ -58,9 +58,6 @@ import org.apache.cassandra.schema.KeyspaceParams;
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.WrappedRunnable;
-import org.json.simple.JSONArray;
-import org.json.simple.JSONObject;
-import org.json.simple.parser.JSONParser;
 
 import static junit.framework.Assert.assertNotNull;
 import static org.apache.cassandra.db.ColumnFamilyStore.FlushReason.UNIT_TESTS;
diff --git a/test/unit/org/apache/cassandra/db/DirectoriesTest.java b/test/unit/org/apache/cassandra/db/DirectoriesTest.java
index 4d1e3e136a..a8599e837f 100644
--- a/test/unit/org/apache/cassandra/db/DirectoriesTest.java
+++ b/test/unit/org/apache/cassandra/db/DirectoriesTest.java
@@ -33,8 +33,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.Callable;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.function.Supplier;
diff --git a/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java b/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java
index 72a9bbaf4e..5744637d61 100644
--- a/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java
+++ b/test/unit/org/apache/cassandra/fql/FullQueryLoggerTest.java
@@ -49,8 +49,6 @@ import org.apache.cassandra.exceptions.ConfigurationException;
 import org.apache.cassandra.fql.FullQueryLogger.Query;
 import org.apache.cassandra.fql.FullQueryLogger.Batch;
 import org.apache.cassandra.cql3.statements.BatchStatement.Type;
-import org.apache.cassandra.fql.FullQueryLogger.Batch;
-import org.apache.cassandra.fql.FullQueryLogger.Query;
 import org.apache.cassandra.io.util.File;
 import org.apache.cassandra.io.util.FileUtils;
 import org.apache.cassandra.service.ClientState;
